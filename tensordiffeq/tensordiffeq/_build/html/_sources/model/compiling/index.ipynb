{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PINN Collocation Solver\n",
    "\n",
    "Once you've defined your problem, it must be compiled such that TensorDiffEq can build the loss function described\n",
    "by the boundary conditions, initial conditions, and physics defined in the previous sections.\n",
    "\n",
    "### Layer Sizes\n",
    "\n",
    "Here is where we will define the neural network size and depth. Currently, most PINN approaches use dense fully connected\n",
    "neural networks for function approximation. Fully-connected Neural Networks have some level of theoretical backing\n",
    "that they will converge to a solution of the underlying function {cite}`pinkus1999approximation,chen1993approximations`, and this\n",
    "theoretical backing has extended into the PINN framework {cite}`shin2020convergence`. With that being said, currently the only type of network supported in\n",
    "TensorDiffEq is the fully-connected MLP network.\n",
    "\n",
    "TensorDiffEq uses the [Keras API](https://keras.io/) for neural network construction. All you need to do is define a list of layer\n",
    "sizes for your neural network. So, for a network with an `[x,t]` input, 4 layers deep, with 128 nodes, one would define\n",
    "a layer size list of `[2,128,128,1]`.\n",
    "\n",
    "For our problem we have been building in the previous sections, we can define layer sizes as such:\n",
    "\n",
    "```{code} python\n",
    "layer_sizes = [2, 128, 128, 128, 128, 1]\n",
    "```\n",
    "\n",
    "Or, if your problem is a function of `[x,y,t]`, then you could define the exact same network with an input layer with 3 nodes, i.e.\n",
    "\n",
    "```{code} python\n",
    "layer_sizes = [3, 128, 128, 128, 128, 1]\n",
    "```\n",
    "\n",
    "### Compile the Model\n",
    "\n",
    "In order to compile the model, we first initialize the model we are interested in. Currently, forward solutions of PINNs are performed by\n",
    "the `CollocationSolverND()` method.\n",
    "\n",
    "#### Collocation Solver\n",
    "\n",
    "The primary method of solving forward problems in TensorDiffEq is the collocation solver. This methodology identifies points\n",
    "in the domain of the problem and collocates them to the solution via a loss function. Therefore, this is a natural application\n",
    "for a neural network function approximation.\n",
    "\n",
    "##### Instantiate the Model\n",
    "\n",
    "The `CollocationSolverND()` solver can be initialized in the following way:\n",
    "\n",
    "```{code} python\n",
    "CollocationSolverND(assimilate=False)\n",
    "```\n",
    "\n",
    "Args:\n",
    "- `assimilate` - a `bool` that describes whether the `CollocationSolverND` will be used for data assimilation\n",
    "\n",
    "Note that very little in the solver is truly initialized when creating the `CollocationSolverND` instance, most comes later in the `compile` call.\n",
    "\n",
    "##### Methods\n",
    "\n",
    "```{code} python\n",
    "compile(layer_sizes, f_model, domain, bcs,\n",
    "    isAdaptive=False,\n",
    "    col_weights=None,\n",
    "    u_weights=None,\n",
    "    g=None,\n",
    "    dist=False)\n",
    "```\n",
    "\n",
    "Args:\n",
    "- `layer_sizes` - a `list` of `ints` describing the size of the input, hidden, and output layers of the FC MLP network\n",
    "- `f_model` - a `func` describing the physics of the problem. More info is provided in [this section](../../physics/index.ipynb)\n",
    "- `domain` - a `domain` object containing the collocation points, defined further [here](../../domain/index.ipynb)\n",
    "- `bcs` - a `list` of BCs describing the problem\n",
    "- `isAdaptive` - a `bool` describing whether the problem is solved adaptively using the [SA-PINN](https://arxiv.org/pdf/2009.04544.pdf)\n",
    "- `col_weights` - a `tf.Variable` object containing the vector of collocation weights used in self-adaptive training, if enabled via `isAdaptive`\n",
    "- `u_weights` - a `tf.Variable` object containing the vector of initial boundary weights used in self-adaptive training, if enabled via `isAdaptive`\n",
    "- `g` - a `func` describing the lambda function described in the [SA-PINN framework](https://arxiv.org/pdf/2009.04544.pdf). This defaults to squaring the collocation weights if not explicitly defined.\n",
    "Only applicable if `isAdaptive` is enabled.\n",
    "- `dist` - a `bool` enabling distributed training across multiple GPUs\n",
    "\n",
    "Model compilation is truly where the rubber meets the road in defining an inference model in TensorDiffEq. We compile the model using the `compile` method on the\n",
    "`CollocationSolverND` method. This will build out the loss function in the solver by iterating through the [BCs](../../ic-bc/bc/index.ipynb) and the [IC](../../ic-bc/ic/index.ipynb)\n",
    "that define your problem. The compile function will also pull in the collocation points and optimize your `f_model` function for running in graph-mode in Tensorflow.\n",
    "\n",
    "```{code} python\n",
    "fit(tf_iter, newton_iter,\n",
    "    batch_sz = None\n",
    "    newton_eager = True)\n",
    "```\n",
    "\n",
    "Args:\n",
    "- `tf_iter` - an `int` dictating the number of iterated for the selected tensorflow optimizer\n",
    "- `newton_iter` - and `int` dictating the number of L-BFGS iterations to be completed following the `tf_iter` iterations\n",
    "\n",
    "```{bibliography} ../../references.bib\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}